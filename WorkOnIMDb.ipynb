{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WorkOnIMDb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wtagsOMfN4yJ","colab_type":"code","outputId":"d7f7228f-6d83-4a08-fc03-8a2a6776859e","executionInfo":{"status":"ok","timestamp":1577363644554,"user_tz":-480,"elapsed":1331,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# please skip this grid as the experiment was conducted on Google Colaboratory.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2FO7CxWZiam","colab_type":"code","outputId":"68e24d63-8b93-450c-f267-31cec62c9a4f","executionInfo":{"status":"ok","timestamp":1577363647907,"user_tz":-480,"elapsed":1095,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# please skip this grid as the experiment was conducted on Google Colaboratory.\n","cd drive/My Drive/Colab Notebooks/AML-Coursework/IMDb"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/AML-Coursework/IMDb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3lOr9lZrZ8fs","colab_type":"text"},"source":["#0. Import dependencies"]},{"cell_type":"code","metadata":{"id":"GEsQ3kboZ_-q","colab_type":"code","outputId":"944bce88-e406-438d-ce4d-7624a75e7287","executionInfo":{"status":"ok","timestamp":1577363651728,"user_tz":-480,"elapsed":1704,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import sklearn\n","import operator\n","import random\n","import math\n","nltk.download('stopwords') # If needed\n","nltk.download('punkt') # If needed\n","nltk.download('wordnet') # If needed"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NSJGGR5ocXNm","colab_type":"text"},"source":["#1. Load data"]},{"cell_type":"code","metadata":{"id":"03JufLOZbfJt","colab_type":"code","outputId":"3c236401-772b-4647-a995-8cbaa7f93ee0","executionInfo":{"status":"ok","timestamp":1577363655095,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["pos_train = pd.read_csv('train/imdb_train_pos.txt', sep=\"\\n\", header=None)\n","neg_train = pd.read_csv('train/imdb_train_neg.txt', sep=\"\\n\", header=None)\n","pos_train = pos_train.iloc[:,0].as_matrix()\n","neg_train = neg_train.iloc[:,0].as_matrix()\n","\n","pos_dev = pd.read_csv('dev/imdb_dev_pos.txt', sep=\"\\n\", header=None)\n","neg_dev = pd.read_csv('dev/imdb_dev_neg.txt', sep=\"\\n\", header=None)\n","pos_dev = pos_dev.iloc[:,0].as_matrix()\n","neg_dev = neg_dev.iloc[:,0].as_matrix()\n","\n","pos_test = pd.read_csv('test/imdb_test_pos.txt', sep=\"\\n\", header=None)\n","neg_test = pd.read_csv('test/imdb_test_neg.txt', sep=\"\\n\", header=None)\n","pos_test = pos_test.iloc[:,0].as_matrix()\n","neg_test = neg_test.iloc[:,0].as_matrix()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  del sys.path[0]\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oi065q1IcKG3","colab_type":"code","outputId":"f06599b2-908f-48ec-83f0-cf8ccab068bb","executionInfo":{"status":"ok","timestamp":1577363659215,"user_tz":-480,"elapsed":1144,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":154}},"source":["print(pos_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['For fans of Chris Farley, this is probably his best film. David Spade plays the perfect cynical, sarcastic yin to Farley\\'s \"Baby Huey\" yang. Farley achieves strokes of comic genius in his monologues, like the \"Let\\'s say you\\'re driving along the road with your family...\" bit, the \"Jo-Jo the Idiot Circus Boy with a pretty new pet, (his possible sale)\" speech, or the \"Glue-sniffing Guarantee fairy\" brake pad sale. The sappy moments in the film contrast sharply with Farley and Spade\\'s shenanigans. Even after many viewings, it\\'s still fun to see Farley pour everything he had into the role. \"Richard, what\\'s HAPPENING to me?!?!\"'\n"," \"Fantastic, Madonna at her finest, the film is funny and her acting is brilliant. It may have been made in the 80's but it has all the qualities of a modern Hollywood Block-buster. I love this film and i think its totally unique and will cheer up any droopy person within a matter of minutes. Fantastic.\"\n"," \"From a perspective that it is possible to make movies that are not offensive to people with strong moral values, this one is definitely worthwhile. This is the second Bruce Willis film in a row that manages to tell its story with no nudity, off-color humor, profanity, or gratuitous violence. (I refer of course to The Sixth Sense.) Both movies are engaging on more than one level. This one is appropriate for children as well, although as others have pointed out, it isn't a flick FOR kids. <br /><br />I was bothered that the time travel device that drives this plot is never explained, except that we know Russell himself initiates it as a 70 year old. Also, why does his dying mother have to come to school to get him when he wins the fight; why, if as his older self says, he has to fight that kid again and again for the next few years does his mother not have to come and get him every time, and why he doesn't learn to kick butt in the process. I also found the score rather annoying and not always appropriate to the action on stage. <br /><br />Good use of the red plane as metaphor, however.\"\n"," ...\n"," \"This is a good film for die-hard Chucky fans. Okay I'm sure it's not as good as what the Child's Play movie were like, but this can get really funny and enjoyable, Chucky's laughs are hilarious.<br /><br />(SPOILERS)<br /><br />Now not one doll, but two, meaning double the impact, Jennifer Tilly played the part really well and definitely pulled off the best kill of the movie.<br /><br />If you have seen the Child's Play movies this would be a worthy film on your Chucky collection, but if you've never seen the Child's play movies before, this'll will be a new start. Of course you'll not have a clue on how Chucky got into his current state (cause I'm not telling you) but you'll figure out why Chucky is very popular.<br /><br />Overall a very enjoyable movie.\"\n"," 'Based on the book \"Space Vampires\" by Colin Wilson. This is (in my humble opinion) one of the best pieces of Sci-Fi Horror to come out of the eighties. The effects (done by ILM) still hold up by todays standards. The actors are mostly British and being british seem to give this film a greater depth of realism.<br /><br /> The film was panned by the critics and sadly failed to do well at the box office on both sides of the atlantic. Tobe Hooper blamed the promotional work that was done before its release as the main cause for it\\'s low takings. But for whatever reason, it still does not detract from the fact that this is an excellent film with a great cast and well-paced plot.<br /><br /> Not to be over-looked.'\n"," 'That\\'s a bad, raunchy, predictable, tacky, salacious soap opera? \"The Best of Everything\" is just such a guilty pleasure for me, something along the lines of \"Valley of the Dolls\". I mean, \"Best\" has everything. Somebody gets pregnant out of wedlock (when\\'s the last time you heard THAT phrase?), there are affairs everywhere, drinking, backstabbing, jealousy, and even a tragic but not altogether unexpected death.<br /><br />Caroline Bender (Hope Lange) and Mike Rice (the delicious Stephen Boyd) are the centerpieces of the goings-on. Their chemistry is immediate and is the glue that keeps this film from becoming too fragmented.<br /><br />Suzy Parker, the off-the-chart gorgeous ex-fashion model, appears as Gregg Adams, an aspiring stage actress, a role that, according to any biography I\\'ve ever read about her, was apparently not much of a stretch. But Parker does a surprisingly credible job here, more than holding her own in a couple of scenes opposite Louis Jourdan who plays David Savage. Jourdan probably took this role for the money and the special screen credit because he was clearly headed down the aging star/has-been road.<br /><br />Diane Baker is fine as naive, gullible April Morrison, Martha Hyer as Barbara Lemont has a particularly juicy storyline, and film legend Joan Crawford chews her usual serving of scenery as Amanda Farrow, another role, like Jourdan\\'s Savage, that Crawford likely took for the paycheck.<br /><br />There is some obviously dated dialog and plot devices (this IS 1959), and the predictability of the soap opera genre. But if you like a good soap as I do, \"The Best of Everything\" will more than satisfy you.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W7W8p1a7cNT_","colab_type":"code","outputId":"e32de888-b74c-4961-8ff2-20727743af96","executionInfo":{"status":"ok","timestamp":1577363661883,"user_tz":-480,"elapsed":1186,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":154}},"source":["print(neg_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[\"A terrible deception: controversial film, winner of the Teddy in Berlin 2003, Mil nubes de paz turned out to be a fiasco. The actors are all reciting (well, they are not exactly actors); the film tried to be a high bet but ends up being a doubtful bet: it stays in the superficiality of two guys kissing and a guy whose lover is gone; it has no purpose: nothing to do with the homo-sexuality presented in other films (e.g. Before Night Falls (2000) by Julian Schnabel). Technically the only thing that works is the photography; otherwise, the camera is put in strange angles (to make it more `art-film') and the whole film runs in a black and white atmosphere. The film is so pretentious that bothers. I mean, it's good to be pretentious when you have talent to support it. Or maybe it is that it's so art-cinema that it's incomprehensible. The story flows slowly, slowly, slowly. To me, more form than essence. Superb edition? It was good. Superb direction? Don't think so: the film is weak. It was an interesting project. It's a shame. It's a flaw. One star out of four.\"\n"," 'Well I guess I know the answer to that question. For the MONEY! We have been so bombarded with Cat In The Hat advertising and merchandise that we almost believe there has to be something good about this movie. I admit, I thought the trailers looked bad, but I still had to give it a chance. Well I should have went with my instincts. It was a complete piece Hollywood trash. Once again proving that the average person can be programed into believing anything they say is good, must be good. Aside from the insulting fact that the film is only about 80 minutes long, it obviously started with a moth eaten script. It\\'s chock full of failed attempts at senseless humor, and awful pastel sceneries. It jumps all over the universe with no destination nor direction. This is then compounded with, ............................yes I\\'ll say it, BAD ACTING! I couldn\\'t help but feel like I was watching \"Coffee Talk\" on SNL every time Mike Myers opened his mouth. Was the Cat intended to be a middle aged Jewish woman? Spencer Breslin and Dakota Fanning were no prize either, but Mr. Myers should disappear under a rock somewhere until he\\'s ready to make another Austin Powers movie. F-, no stars, 0 on a scale of 1-10. Save your money!'\n"," 'I really liked the movie \\'The Emporer\\'s New Groove\\', but watching this was like coming home and seeing your wife having \"relations\" with a llama. Seriously, this movie was bad. It\\'s like Club Dread after Super Troopers. I am supposed to write 10 lines, but I don\\'t even know what else to say. I laughed a couple of times, but only because I was drinking. A movie like that should at least be funny when your drunk. It was not. Maybe llamas are just funny and regular cartoon people aren\\'t. Either way, just stick with The Emporer\\'s New Groove if you want a funny, cartoon, llama-themed movie. Line 10 is this line right here.'\n"," ...\n"," 'It pains me to see an awesome movie turn into some lame, repetitive and lazy series. It is filled with plot holes and the plot is confusing, in a BAD way. Whoever the prick writers were that decided to turn such a great movie into this garbage should have done some research, instead of filling it with one-liners and hollow new characters, and the classic jokes from the first movie OVER AND OVER AND OVER AND OVER AGAIN. Sure they get a little creative, but its like seeing the same episode with a small twist. Pretty much like listening to Creed, or Nickleback. Kuzco has to prevent himself from failing, Yzma has a complicated plan, but decides to go the easy way to save time and just use a potion, someone questions the monkey and the bug, Bucky appears in the background, Kuzco flirts with Malina, she disses him without sounding like a bitch, Yzma disguises herself as \"Principal Amzy\" and she calls Kronk, and he forgets that she is Yzma. I admit, this show does have it\\'s moments. Another problem is the fact that Yzma looks younger and Pacha looks....weird. Also, no one can replace David Spade and John Goodman! Their the ones who made Kuzco and Pacha Kuzco and Pacha! Sorry, but i give this show two thumbs down.'\n"," 'Grande Ecole is not an artful exploration of mixed sexuality but, if you\\'re in need of it, a movie for an X-rated channel. Although I suspect there\\'s nothing in this movie to spoil for a willing viewer, the plot is simply an excuse for male-to-female and male-to-male couplings set in the unconvincing context of a competition between a Parisian school for future CEOs and a major school for those seeking higher degrees in the liberal arts. There\\'s likewise a frisson of cultural clash between high status and lower status French youth, plus a societal conflict involving native Frenchmen and Arab immigrants from North Africa. All that\\'s missing is a female-to-female coupling, which could easily have been arranged with no more than a slight twist in the plot.<br /><br />The acting is at a somewhat higher level than in the usual pornographic movie -- but \"Grande Ecole\" is, to be blunt about it, no more than pornography with artistic aspirations. I\\'m not offended by the sex. It\\'s just repetitive and, before long, boring. Where\\'s the Hays Office when you really need it?'\n"," \"I felt like I was watching an example of how not to make a movie. I think the director filmed it in his back yard! There was no real plot. <br /><br />Terrible script.<br /><br />Terrible acting.<br /><br />The worst production I have ever witnessed. A couple of bad CG effects and then the rest of the movies was spent walking around in what looked like a junk yard.<br /><br />I don't normally write reviews to movies but was moved to warn everyone about this one.<br /><br />Life is to short to waste your time with this movie!\"]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pdvg7CGUwPc5","colab_type":"text"},"source":["#2. Preprocessing and deriving the frequency-based feature (TF-IDF)"]},{"cell_type":"markdown","metadata":{"id":"PeqiHnIQwaUB","colab_type":"text"},"source":["2.1 setup elements for tokenization"]},{"cell_type":"code","metadata":{"id":"RYstO2Nd4NjA","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","def get_list_tokens(string):\n","  sentence_split=nltk.tokenize.sent_tokenize(string)\n","  list_tokens=[]\n","  for sentence in sentence_split:\n","    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n","    for token in list_tokens_sentence:\n","      list_tokens.append(lemmatizer.lemmatize(token).lower())\n","  return list_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPODAr8Ywwm1","colab_type":"text"},"source":["2.2 setup stopwords and derive the vocabulary"]},{"cell_type":"code","metadata":{"id":"ivLwGju14QfK","colab_type":"code","colab":{}},"source":["# get the english stopwords list from nltk\n","stopwords=set(nltk.corpus.stopwords.words('english'))\n","# add more words to the stopword list\n","stopwords.add(\".\")\n","stopwords.add(\",\")\n","stopwords.add(\"--\")\n","stopwords.add(\"``\")\n","stopwords.add(\"/\")\n","stopwords.add(\">\")\n","stopwords.add(\"<\")\n","stopwords.add(\"br\")\n","stopwords.add(\"'s\")\n","stopwords.add(\")\")\n","stopwords.add(\"(\")\n","stopwords.add(\"''\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8uh0Ux14-Ry","colab_type":"code","colab":{}},"source":["# Now we create a frequency dictionary with all words in the dataset\n","# This can take a few minutes depending on your computer, since we are processing more than ten thousand sentences\n","\n","dict_word_frequency = {}\n","\n","for pos_review in pos_train:\n","  sentence_tokens = get_list_tokens(pos_review)\n","  for word in sentence_tokens:\n","    if word in stopwords: continue\n","    if word not in dict_word_frequency: dict_word_frequency[word]=1\n","    else: dict_word_frequency[word]+=1\n","\n","for neg_review in neg_train:\n","  sentence_tokens = get_list_tokens(neg_review)\n","  for word in sentence_tokens:\n","    if word in stopwords: continue\n","    if word not in dict_word_frequency: dict_word_frequency[word]=1\n","    else: dict_word_frequency[word]+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1zZSBWt5_Xt","colab_type":"code","outputId":"928321f1-87b2-40be-a6f5-5b8586aa4388","executionInfo":{"status":"ok","timestamp":1577363727362,"user_tz":-480,"elapsed":1109,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# Now we create a sorted frequency list with the top 1000 words, using the function \"sorted\". Let's see the 15 most frequent words\n","sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n","i=0\n","for word, frequency in sorted_list[:15]:\n","  i+=1\n","  print (str(i)+\". \"+word+\" - \"+str(frequency))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1. movie - 29647\n","2. wa - 29577\n","3. film - 26929\n","4. n't - 19639\n","5. one - 15987\n","6. ! - 14847\n","7. like - 11876\n","8. ha - 9893\n","9. ? - 9593\n","10. time - 8589\n","11. good - 8376\n","12. character - 8318\n","13. would - 7867\n","14. ... - 7722\n","15. even - 7321\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qBBsFX9h6DMu","colab_type":"code","colab":{}},"source":["# Finally, we create our vocabulary based on the sorted frequency list \n","vocabulary=[]\n","for word,frequency in sorted_list:\n","  vocabulary.append(word)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1qNV-IRSxl6L","colab_type":"text"},"source":["2.3 transform raw text to vector with the derived vocabulary"]},{"cell_type":"code","metadata":{"id":"CHgbJ6817GfJ","colab_type":"code","colab":{}},"source":["def get_vector_text(list_vocab,string):\n","  vector_text=np.zeros(len(list_vocab))\n","  list_tokens_string=get_list_tokens(string)\n","  for i, word in enumerate(list_vocab):\n","    if word in list_tokens_string:\n","      vector_text[i]=list_tokens_string.count(word)\n","  return vector_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayeobXeX7SUi","colab_type":"code","colab":{}},"source":["X_train=[]\n","Y_train=[]\n","for pos_review in pos_train:\n","  vector_pos_review=get_vector_text(vocabulary,pos_review)\n","  X_train.append(vector_pos_review)\n","  Y_train.append(1)\n","for neg_review in neg_train:\n","  vector_neg_review=get_vector_text(vocabulary,neg_review)\n","  X_train.append(vector_neg_review)\n","  Y_train.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKNwGXVh8Z01","colab_type":"code","colab":{}},"source":["X_dev=[]\n","Y_dev=[]\n","for pos_review in pos_dev:\n","  vector_pos_review=get_vector_text(vocabulary,pos_review)\n","  X_dev.append(vector_pos_review)\n","  Y_dev.append(1)\n","for neg_review in neg_dev:\n","  vector_neg_review=get_vector_text(vocabulary,neg_review)\n","  X_dev.append(vector_neg_review)\n","  Y_dev.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfU6Xjon8Kda","colab_type":"code","colab":{}},"source":["X_test=[]\n","Y_test=[]\n","for pos_review in pos_test:\n","  vector_pos_review=get_vector_text(vocabulary,pos_review)\n","  X_test.append(vector_pos_review)\n","  Y_test.append(1)\n","for neg_review in neg_test:\n","  vector_neg_review=get_vector_text(vocabulary,neg_review)\n","  X_test.append(vector_neg_review)\n","  Y_test.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70LwiWT1y3c0","colab_type":"text"},"source":["2.4 transform the raw frequence vectors to TF-IDF feature"]},{"cell_type":"code","metadata":{"id":"b1-ZL0Pj1Onx","colab_type":"code","colab":{}},"source":["TF_train = []\n","for f_vector in X_train:\n","  TF_train.append(f_vector/sum(f_vector))\n","\n","X_train_temp = np.asarray(X_train)\n","IDF_vec = []\n","for i in range(X_train_temp.shape[1]):\n","\n","  count_temp = 0\n","  for j in range(X_train_temp.shape[0]):\n","    if X_train_temp[j,i] != 0:\n","      count_temp += 1\n","\n","  IDF_vec.append(math.log(X_train_temp.shape[0]/(count_temp+1)))\n","\n","TF_IDF_train = []\n","for TF_vector in TF_train:\n","  TF_IDF_train.append(TF_vector * IDF_vec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhytr3Vc-lEd","colab_type":"code","colab":{}},"source":["TF_dev = []\n","for f_vector in X_dev:\n","  TF_dev.append(f_vector/sum(f_vector))\n","\n","TF_IDF_dev = []\n","for TF_vector in TF_dev:\n","  TF_IDF_dev.append(TF_vector * IDF_vec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8twRDAF-5eV","colab_type":"code","colab":{}},"source":["TF_test = []\n","for f_vector in X_test:\n","  TF_test.append(f_vector/sum(f_vector))\n","\n","TF_IDF_test = []\n","for TF_vector in TF_test:\n","  TF_IDF_test.append(TF_vector * IDF_vec)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"070i_P83AKFh","colab_type":"text"},"source":["#3. Extraction of the 2nd feature - N-gram (N=2) "]},{"cell_type":"markdown","metadata":{"id":"fQOKzIM3Rxt-","colab_type":"text"},"source":["3.1 learn 2-gram model from train set and extract 2-gram feature for the train set "]},{"cell_type":"code","metadata":{"id":"eek9kYQS_8iw","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","train_drop_stopwords = []\n","for pos_review in pos_train:\n","  sentence_tokens = get_list_tokens(pos_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  train_drop_stopwords.append(new_sentence) \n","\n","for neg_review in neg_train:\n","  sentence_tokens = get_list_tokens(neg_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  train_drop_stopwords.append(new_sentence)\n","\n","twoGram = CountVectorizer(min_df=1, ngram_range=(2,2))\n","twoGram_train = twoGram.fit_transform(train_drop_stopwords)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVmGYBMjSASY","colab_type":"text"},"source":["3.2 extract 2-gram feature for the development set "]},{"cell_type":"code","metadata":{"id":"5lolU6gDKvz3","colab_type":"code","colab":{}},"source":["dev_drop_stopwords = []\n","for pos_review in pos_dev:\n","  sentence_tokens = get_list_tokens(pos_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  dev_drop_stopwords.append(new_sentence)\n","\n","for neg_review in neg_dev:\n","  sentence_tokens = get_list_tokens(neg_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  dev_drop_stopwords.append(new_sentence)\n","\n","twoGram_dev = twoGram.transform(dev_drop_stopwords)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNaIz0zlSGIr","colab_type":"text"},"source":["3.3 extract 2-gram feature for the test set "]},{"cell_type":"code","metadata":{"id":"loRRt9EaO5yY","colab_type":"code","colab":{}},"source":["test_drop_stopwords = []\n","for pos_review in pos_test:\n","  sentence_tokens = get_list_tokens(pos_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  test_drop_stopwords.append(new_sentence)\n","\n","for neg_review in neg_test:\n","  sentence_tokens = get_list_tokens(neg_review)\n","  new_sentence = ''\n","  for i, word in enumerate(sentence_tokens):\n","    if word in stopwords: continue\n","    if word not in vocabulary: continue\n","    if i == 0:\n","      new_sentence = new_sentence + word\n","    else:\n","      new_sentence = new_sentence + ' ' + word\n","  test_drop_stopwords.append(new_sentence)\n","\n","twoGram_test = twoGram.transform(test_drop_stopwords)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrArT5aYSad4","colab_type":"text"},"source":["#4. Extraction of the 3nd feature - Word2vec "]},{"cell_type":"markdown","metadata":{"id":"yG3KJ6c0fXG8","colab_type":"text"},"source":["4.1 learn Word2vec model from train set and extract Word2vec feature for the train set "]},{"cell_type":"code","metadata":{"id":"flNW_2_qShBf","colab_type":"code","outputId":"ad13a530-b5e1-4577-b81a-14c91df783e5","executionInfo":{"status":"ok","timestamp":1577371158046,"user_tz":-480,"elapsed":57641,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from gensim.models import Word2Vec\n","\n","W2V_base = Word2Vec(train_drop_stopwords, min_count=5, size=500, workers=4)\n","W2V_train = []\n","for sentence in train_drop_stopwords:\n","  temp_vector = np.zeros(500)\n","  count = 0\n","  for word in sentence:\n","    try:\n","      temp_vector += W2V_base[word]\n","      count += 1\n","    except:\n","      pass\n","  W2V_train.append(temp_vector/count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"uuBd6klPfdrh","colab_type":"text"},"source":["4.2 extract Word2vec feature for the development set"]},{"cell_type":"code","metadata":{"id":"rh5TpUwpUFPA","colab_type":"code","outputId":"7fd51166-f145-4187-f3cd-ab019ad0de6d","executionInfo":{"status":"ok","timestamp":1577371195306,"user_tz":-480,"elapsed":14643,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["W2V_dev = []\n","for sentence in dev_drop_stopwords:\n","  temp_vector = np.zeros(500)\n","  count = 0\n","  for word in sentence:\n","    try:\n","      temp_vector += W2V_base[word]\n","      count += 1\n","    except:\n","      pass\n","  W2V_dev.append(temp_vector/count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  import sys\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"cjTBg44GfkWr","colab_type":"text"},"source":["4.3 extract Word2vec feature for the test set"]},{"cell_type":"code","metadata":{"id":"UVfwWL3AfLRU","colab_type":"code","outputId":"14716d47-ccd5-4d48-e75b-2cc335e2e33e","executionInfo":{"status":"ok","timestamp":1577371263595,"user_tz":-480,"elapsed":14674,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["W2V_test = []\n","for sentence in test_drop_stopwords:\n","  temp_vector = np.zeros(500)\n","  count = 0\n","  for word in sentence:\n","    try:\n","      temp_vector += W2V_base[word]\n","      count += 1\n","    except:\n","      pass\n","  W2V_test.append(temp_vector/count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  import sys\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"F1PrfeGzhUYb","colab_type":"text"},"source":["#5. Feature selection"]},{"cell_type":"code","metadata":{"id":"YstDOtJYhW_O","colab_type":"code","colab":{}},"source":["from sklearn.feature_selection import chi2\n","from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectKBest"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCk78257j9Wn","colab_type":"text"},"source":["5.1 feature selection for TF-IDF feature"]},{"cell_type":"code","metadata":{"id":"8dGS3rrfhXxS","colab_type":"code","outputId":"51a4b653-771e-4994-f313-8159a815b44f","executionInfo":{"status":"ok","timestamp":1577369705310,"user_tz":-480,"elapsed":872,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["TF_IDF_train_fix = np.asarray(TF_IDF_train)\n","TF_IDF_dev_fix = np.asarray(TF_IDF_dev)\n","TF_IDF_test_fix = np.asarray(TF_IDF_test)\n","\n","Y_train_fix = np.asarray(Y_train)\n","Y_dev_fix = np.asarray(Y_dev)\n","Y_test_fix = np.asarray(Y_test)\n","\n","TF_IDF_select = SelectKBest(chi2, k=500).fit(TF_IDF_train_fix, Y_train_fix)\n","TF_IDF_train_selected = TF_IDF_select.transform(TF_IDF_train_fix)\n","TF_IDF_dev_selected = TF_IDF_select.transform(TF_IDF_dev_fix)\n","TF_IDF_test_selected = TF_IDF_select.transform(TF_IDF_test_fix)\n","\n","print (\"Size original training matrix: \"+str(TF_IDF_train_fix.shape))\n","print (\"Size new training matrix: \"+str(TF_IDF_train_selected.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size original training matrix: (15000, 1000)\n","Size new training matrix: (15000, 500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iuBKT98ZkLhi","colab_type":"text"},"source":["5.2 feature selection for 2-gram feature"]},{"cell_type":"code","metadata":{"id":"KDxVOCcdkRVf","colab_type":"code","outputId":"02ec6339-fc13-43ee-8e2a-dbcffa2dc203","executionInfo":{"status":"ok","timestamp":1577370426859,"user_tz":-480,"elapsed":6097,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["twoGram_select = SelectKBest(chi2, k=1000).fit(twoGram_train, Y_train_fix)\n","twoGram_train_selected = twoGram_select.transform(twoGram_train)\n","twoGram_dev_selected = twoGram_select.transform(twoGram_dev)\n","twoGram_test_selected = twoGram_select.transform(twoGram_test)\n","\n","print (\"Size original training matrix: \"+str(twoGram_train.shape))\n","print (\"Size new training matrix: \"+str(twoGram_train_selected.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size original training matrix: (15000, 325515)\n","Size new training matrix: (15000, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nmIsJK9ImwRz","colab_type":"text"},"source":["5.3 feature selection for Word2vec feature"]},{"cell_type":"code","metadata":{"id":"043fEejSm2rR","colab_type":"code","outputId":"d1a499ce-9b02-4e9c-c29e-8ac956091cc9","executionInfo":{"status":"ok","timestamp":1577371270957,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["W2V_train_fix = np.asarray(W2V_train)\n","W2V_dev_fix = np.asarray(W2V_dev)\n","W2V_test_fix = np.asarray(W2V_test)\n","\n","W2V_select = SelectKBest(f_classif, k=300).fit(W2V_train_fix, Y_train_fix)\n","W2V_train_selected = W2V_select.transform(W2V_train_fix)\n","W2V_dev_selected = W2V_select.transform(W2V_dev_fix)\n","W2V_test_selected = W2V_select.transform(W2V_test_fix)\n","\n","print (\"Size original training matrix: \"+str(W2V_train_fix.shape))\n","print (\"Size new training matrix: \"+str(W2V_train_selected.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size original training matrix: (15000, 500)\n","Size new training matrix: (15000, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LF-3GoPCp6es","colab_type":"text"},"source":["#6. Feature combination and further selection"]},{"cell_type":"code","metadata":{"id":"DXyRFSOvxuHV","colab_type":"code","colab":{}},"source":["twoGram_train_selected = np.asarray(twoGram_train_selected.todense())\n","twoGram_dev_selected = np.asarray(twoGram_dev_selected.todense())\n","twoGram_test_selected = np.asarray(twoGram_test_selected.todense())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"so4CpGWoqAIQ","colab_type":"code","outputId":"fdd948d8-0f5c-44db-8fda-21ea3142438a","executionInfo":{"status":"ok","timestamp":1577373499473,"user_tz":-480,"elapsed":2242,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["COM_train = np.column_stack((TF_IDF_train_selected, twoGram_train_selected, W2V_train_selected))\n","COM_dev = np.column_stack((TF_IDF_dev_selected, twoGram_dev_selected, W2V_dev_selected))\n","COM_test = np.column_stack((TF_IDF_test_selected, twoGram_test_selected, W2V_test_selected))\n","\n","COM_select = SelectKBest(f_classif, k=1000).fit(COM_train, Y_train_fix)\n","COM_train_selected = COM_select.transform(COM_train)\n","COM_dev_selected = COM_select.transform(COM_dev)\n","COM_test_selected = COM_select.transform(COM_test)\n","\n","print (\"Size original training matrix: \"+str(COM_train.shape))\n","print (\"Size new training matrix: \"+str(COM_train_selected.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size original training matrix: (15000, 1800)\n","Size new training matrix: (15000, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zkwoMaI21P0z","colab_type":"text"},"source":["#7. Train the classification model and evaluation on the development set"]},{"cell_type":"markdown","metadata":{"id":"boWm6lR5zAsV","colab_type":"text"},"source":["7.1 train a SVM model"]},{"cell_type":"code","metadata":{"id":"dNLXWPT97dEJ","colab_type":"code","outputId":"494b3591-7387-43b8-d694-64a3058164c5","executionInfo":{"status":"ok","timestamp":1577373836682,"user_tz":-480,"elapsed":183686,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["svm_1st = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n","svm_1st.fit(COM_train_selected, Y_train_fix)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"GXqsce6ZzGZl","colab_type":"text"},"source":["7.2 predict on development set"]},{"cell_type":"code","metadata":{"id":"BL-ONdfv93n0","colab_type":"code","colab":{}},"source":["Y_dev_pred = svm_1st.predict(COM_dev_selected)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naeh-Lizzj-6","colab_type":"text"},"source":["7.3 evaluation with different indexes"]},{"cell_type":"code","metadata":{"id":"0KVWFg8OA3AV","colab_type":"code","outputId":"7bea44c4-93aa-4124-ab79-27a74f67d46a","executionInfo":{"status":"ok","timestamp":1577373923783,"user_tz":-480,"elapsed":1248,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n","\n","precision = precision_score(Y_dev_fix, Y_dev_pred, average='macro')\n","recall = recall_score(Y_dev_fix, Y_dev_pred, average='macro')\n","f1 = f1_score(Y_dev_fix, Y_dev_pred, average='macro')\n","accuracy = accuracy_score(Y_dev_fix, Y_dev_pred)\n","\n","print(precision)\n","print(recall)\n","print(f1)\n","print(accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8379944543148083\n","0.837246122839008\n","0.8372778347071848\n","0.8374\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"enizdL6f0ryB","colab_type":"text"},"source":["#8. Modify the feature selection scheme and re-test on the development set"]},{"cell_type":"markdown","metadata":{"id":"oerP23u51X_B","colab_type":"text"},"source":["8.1 apply a new feature selection and combination scheme"]},{"cell_type":"code","metadata":{"id":"O9A6Ygre02gO","colab_type":"code","outputId":"f2693529-17f4-473a-f2bb-bd3440237f00","executionInfo":{"status":"ok","timestamp":1577375263446,"user_tz":-480,"elapsed":14752,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["COM_train_new = np.column_stack((TF_IDF_train, twoGram_train_selected, W2V_train))\n","COM_dev_new = np.column_stack((TF_IDF_dev, twoGram_dev_selected, W2V_dev))\n","COM_test_new = np.column_stack((TF_IDF_test, twoGram_test_selected, W2V_test))\n","\n","COM_select_new = SelectKBest(f_classif, k=1000).fit(COM_train_new, Y_train_fix)\n","COM_train_selected_new = COM_select_new.transform(COM_train_new)\n","COM_dev_selected_new = COM_select_new.transform(COM_dev_new)\n","COM_test_selected_new = COM_select_new.transform(COM_test_new)\n","\n","print (\"Size original training matrix: \"+str(COM_train_new.shape))\n","print (\"Size new training matrix: \"+str(COM_test_selected_new.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size original training matrix: (15000, 2500)\n","Size new training matrix: (5000, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"URJpPi911eCg","colab_type":"text"},"source":["8.2 train the 2nd model and evaluate it"]},{"cell_type":"code","metadata":{"id":"OPZMcN9W1lP1","colab_type":"code","outputId":"4e15cd48-7e47-4a5c-aed2-e8af9365bb96","executionInfo":{"status":"ok","timestamp":1577375500950,"user_tz":-480,"elapsed":221601,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["svm_2nd = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n","svm_2nd.fit(COM_train_selected_new, Y_train_fix)\n","\n","Y_dev_pred = svm_2nd.predict(COM_dev_selected_new)\n","\n","precision = precision_score(Y_dev_fix, Y_dev_pred, average='macro')\n","recall = recall_score(Y_dev_fix, Y_dev_pred, average='macro')\n","f1 = f1_score(Y_dev_fix, Y_dev_pred, average='macro')\n","accuracy = accuracy_score(Y_dev_fix, Y_dev_pred)\n","\n","print(precision)\n","print(recall)\n","print(f1)\n","print(accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8390555709534369\n","0.8382389743084282\n","0.8382695546920325\n","0.8384\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8GSVKWGd0E52","colab_type":"text"},"source":["#9. Evaluation on the test set and report the final results"]},{"cell_type":"markdown","metadata":{"id":"wZALTER72XM0","colab_type":"text"},"source":["9.1 evaluate the first model on test set"]},{"cell_type":"code","metadata":{"id":"Edzw10VE0JjR","colab_type":"code","outputId":"54140d0d-a8d8-4ef8-fa87-7ebdc5204279","executionInfo":{"status":"ok","timestamp":1577375263445,"user_tz":-480,"elapsed":47015,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["Y_test_pred = svm_1st.predict(COM_test_selected)\n","\n","precision = precision_score(Y_test_fix, Y_test_pred, average='macro')\n","recall = recall_score(Y_test_fix, Y_test_pred, average='macro')\n","f1 = f1_score(Y_test_fix, Y_test_pred, average='macro')\n","accuracy = accuracy_score(Y_test_fix, Y_test_pred)\n","\n","print(precision)\n","print(recall)\n","print(f1)\n","print(accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8367893692714989\n","0.8352137336341974\n","0.8350092707169487\n","0.8352\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lx2YUoCy2jdO","colab_type":"text"},"source":["9.2 evaluate the second model on test set"]},{"cell_type":"code","metadata":{"id":"uA0OoT452iuQ","colab_type":"code","outputId":"1e4dc722-911d-43f9-fcac-f5d0f86d1c1b","executionInfo":{"status":"ok","timestamp":1577375771663,"user_tz":-480,"elapsed":47491,"user":{"displayName":"Tian Lin","photoUrl":"","userId":"08087428425896145451"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["Y_test_pred = svm_2nd.predict(COM_test_selected_new)\n","\n","precision = precision_score(Y_test_fix, Y_test_pred, average='macro')\n","recall = recall_score(Y_test_fix, Y_test_pred, average='macro')\n","f1 = f1_score(Y_test_fix, Y_test_pred, average='macro')\n","accuracy = accuracy_score(Y_test_fix, Y_test_pred)\n","\n","print(precision)\n","print(recall)\n","print(f1)\n","print(accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8359204737036885\n","0.8346125335380054\n","0.8344408977026923\n","0.8346\n"],"name":"stdout"}]}]}